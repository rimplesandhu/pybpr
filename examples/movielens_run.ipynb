{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Recommendation System Training\n",
    "\n",
    "This notebook demonstrates how to train a hybrid matrix factorization recommendation system on the MovieLens 10M dataset using `pybpr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the required packages + set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import hero\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathos.multiprocessing import ProcessPool\n",
    "\n",
    "from pybpr import RecSys, UserItemData, HybridMF\n",
    "from pybpr import bpr_loss, hinge_loss, bpr_loss_v2\n",
    "from pybpr.movielens_downloader import MovieLensDownloader\n",
    "\n",
    "load_dotenv()\n",
    "torch.set_num_threads(1)  # Limit threads per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: https://dev-hero.nrel.gov/ml-model-registry/api/v1/proxy/dev-recommender-system-app\n",
      "Experiment: MovieLens BPR Experiments (ID: 133)\n"
     ]
    }
   ],
   "source": [
    "hero_client = hero.HeroClient()\n",
    "model_registry = hero_client.MLModelRegistry()\n",
    "mlflow = model_registry.get_patched_mlflow()\n",
    "mlflow.set_tracking_uri(model_registry.get_tracking_uri())\n",
    "\n",
    "experiment_name = \"MovieLens BPR Experiments\"\n",
    "experiment = model_registry.read_or_create_experiment(experiment_name)\n",
    "print(f\"Tracking URI: {model_registry.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {experiment.name} (ID: {experiment.experiment_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the data\n",
    "\n",
    "We use `MovieLensDownloader` to download the MovieLens 20M dataset (which includes ratings and tag genome scores). Items (movies) without tag genome data are filtered out, and tags with relevance below 0.8 are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieLensDownloader-INFO-Cache directory: data/movielens\n",
      "MovieLensDownloader-INFO-Dataset ml-20m already exists at data/movielens/ml-20m\n",
      "MovieLensDownloader-INFO-Loaded 20,000,263 ratings\n",
      "MovieLensDownloader-INFO-Loaded 27,278 movies\n",
      "MovieLensDownloader-INFO-Loaded 465,564 tags\n",
      "MovieLensDownloader-INFO-Loaded 27,278 links\n",
      "MovieLensDownloader-INFO-Loaded 11,709,768 genome_scores\n",
      "MovieLensDownloader-INFO-Loaded 1,128 genome_tags\n",
      "MovieLensDownloader-INFO-Loaded 6 datasets: ['ratings', 'movies', 'tags', 'links', 'genome_scores', 'genome_tags']\n",
      "Available keys: ['ratings', 'movies', 'tags', 'links', 'genome_scores', 'genome_tags']\n",
      "Ratings: 19,738,489 rows\n",
      "Tags: 103,052 rows\n",
      "Unique users: 138,493\n",
      "Unique movies: 10,150\n"
     ]
    }
   ],
   "source": [
    "# Download and load the MovieLens 20M dataset (includes genome scores)\n",
    "downloader = MovieLensDownloader(cache_dir='./data', log_level=20)\n",
    "data = downloader.load_dataset_with_tags('ml-20m')\n",
    "\n",
    "print(f\"Available keys: {list(data.keys())}\")\n",
    "assert 'genome_scores' in data, (\n",
    "    \"genome_scores not found in loaded data. \"\n",
    "    \"Check that genome-scores.csv exists in the downloaded dataset directory.\"\n",
    ")\n",
    "\n",
    "# Ratings\n",
    "rdf = data['ratings'].rename(columns={\n",
    "    'userId': 'UserID',\n",
    "    'movieId': 'MovieID',\n",
    "    'rating': 'Rating',\n",
    "    'timestamp': 'Timestamp'\n",
    "})\n",
    "\n",
    "# Tag genome scores (movieId, tagId, relevance)\n",
    "tdf = data['genome_scores'].rename(columns={\n",
    "    'movieId': 'MovieID',\n",
    "    'tagId': 'TagID',\n",
    "    'relevance': 'Relevance'\n",
    "})\n",
    "tdf.drop(index=tdf.index[tdf.Relevance < 0.8], inplace=True)\n",
    "\n",
    "# Filter ratings to only include movies with tag data\n",
    "rdf = rdf[rdf.MovieID.isin(tdf.MovieID.unique())]\n",
    "\n",
    "print(f\"Ratings: {len(rdf):,} rows\")\n",
    "print(f\"Tags: {len(tdf):,} rows\")\n",
    "print(f\"Unique users: {rdf.UserID.nunique():,}\")\n",
    "print(f\"Unique movies: {rdf.MovieID.nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the model building function\n",
    "\n",
    "The `build_recsys` function constructs a `UserItemData` object, configures positive/negative interactions and item features based on the experiment parameters, splits into train/test, and trains a `HybridMF` model using BPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recsys(\n",
    "    rdf,\n",
    "    tdf,\n",
    "    run,\n",
    "    neg_option,\n",
    "    item_option,\n",
    "    n_latent,\n",
    "    learning_rate,\n",
    "    loss_function,\n",
    "    weight_decay,\n",
    "    n_iter=5,\n",
    "    batch_size=1000,\n",
    "    eval_every=10,\n",
    "    eval_user_size=7000,\n",
    "    output_dir=\"/kfs2/projects/zazzle/pybpr/examples/output/movielens/\",\n",
    "):\n",
    "    \"\"\"Build and train a recommendation system.\"\"\"\n",
    "\n",
    "    # Name the run\n",
    "    name = f'{run}_{item_option}_{loss_function.__name__}'\n",
    "    name += f'_{neg_option}'\n",
    "    name += f'_ld{n_latent}_lr{int(learning_rate*1000)}'\n",
    "    name += f'_wd{int(weight_decay*1000)}'\n",
    "    print(f\"Starting process: {name}\", flush=True)\n",
    "\n",
    "    # Build data object\n",
    "    ui = UserItemData(name=name)\n",
    "\n",
    "    ui.add_positive_interactions(\n",
    "        user_ids=rdf.UserID[rdf.Rating >= 4.0],\n",
    "        item_ids=rdf.MovieID[rdf.Rating >= 4.0]\n",
    "    )\n",
    "    if neg_option != 'neg-ignore':\n",
    "        ui.add_negative_interactions(\n",
    "            user_ids=rdf.UserID[rdf.Rating < 4.0],\n",
    "            item_ids=rdf.MovieID[rdf.Rating < 4.0]\n",
    "        )\n",
    "    ui.add_user_features(\n",
    "        user_ids=rdf.UserID.unique(),\n",
    "        feature_ids=rdf.UserID.unique()\n",
    "    )\n",
    "    if item_option == 'metadata':\n",
    "        ui.add_item_features(\n",
    "            item_ids=tdf.MovieID,\n",
    "            feature_ids=tdf.TagID\n",
    "        )\n",
    "    elif item_option == 'indicator':\n",
    "        ui.add_item_features(\n",
    "            item_ids=tdf.MovieID.unique(),\n",
    "            feature_ids=tdf.MovieID.unique()\n",
    "        )\n",
    "    elif item_option == 'both':\n",
    "        ui.add\n",
    "        ui.add_item_features(\n",
    "            item_ids=np.concatenate(\n",
    "                (tdf.MovieID.values, tdf.MovieID.unique())),\n",
    "            feature_ids=np.concatenate(\n",
    "                (tdf.TagID.values, tdf.TagID.max() + tdf.MovieID.unique()))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown item_features type: {item_option}\")\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    ui.train_test_split(\n",
    "        train_ratio_pos=0.8,\n",
    "        train_ratio_neg=0.0 if neg_option == 'neg-test' else 0.8,\n",
    "        show_progress=False\n",
    "    )\n",
    "    print(ui, flush=True)\n",
    "\n",
    "    # Build recommender\n",
    "    recommender = RecSys(\n",
    "        data=ui,\n",
    "        model=HybridMF(ui.n_user_features,\n",
    "                       ui.n_item_features, n_latent=n_latent),\n",
    "        optimizer=partial(\n",
    "            torch.optim.Adam, lr=learning_rate, weight_decay=weight_decay\n",
    "        ),\n",
    "        loss_function=loss_function,\n",
    "        output_dir=os.path.join(output_dir, ui.name),\n",
    "        log_level=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    recommender.fit(\n",
    "        n_iter=n_iter,\n",
    "        batch_size=batch_size,\n",
    "        eval_every=eval_every,\n",
    "        eval_user_size=eval_user_size,\n",
    "        early_stopping_patience=100\n",
    "    )\n",
    "\n",
    "    print(f\"Finished process: {name}\")\n",
    "    return recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the experiment parameter grid\n",
    "\n",
    "We define a grid of hyperparameters to sweep over. Each combination of parameters will be run as a separate experiment. Adjust the grid to expand or narrow the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments to run: 6\n",
      "  [0] item=metadata, neg=neg-ignore, loss=bpr_loss\n",
      "  [1] item=metadata, neg=neg-test, loss=bpr_loss\n",
      "  [2] item=metadata, neg=neg-both, loss=bpr_loss\n",
      "  [3] item=indicator, neg=neg-ignore, loss=bpr_loss\n",
      "  [4] item=indicator, neg=neg-test, loss=bpr_loss\n",
      "  [5] item=indicator, neg=neg-both, loss=bpr_loss\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'run': list(range(1)),\n",
    "    'item_option': ['metadata', 'indicator'],\n",
    "    'n_latent': [64],\n",
    "    'learning_rate': [0.005],\n",
    "    'loss_function': [bpr_loss],\n",
    "    'weight_decay': [0],\n",
    "    'neg_option': ['neg-ignore', 'neg-test', 'neg-both'],\n",
    "}\n",
    "\n",
    "# Generate all parameter combinations\n",
    "all_params = []\n",
    "param_names = list(param_grid.keys())\n",
    "param_values = list(param_grid.values())\n",
    "for values in itertools.product(*param_values):\n",
    "    params = dict(zip(param_names, values))\n",
    "    all_params.append(params)\n",
    "\n",
    "print(f\"Total experiments to run: {len(all_params)}\")\n",
    "for i, p in enumerate(all_params):\n",
    "    print(f\"  [{i}] item={p['item_option']}, neg={p['neg_option']}, loss={p['loss_function'].__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run experiments in parallel\n",
    "\n",
    "We use Python's `multiprocessing.Pool` to run the experiments in parallel across available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 processes\n",
      "Starting process: 0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process: 0_metadata_bpr_loss_neg-test_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_metadata_bpr_loss_neg-test_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_metadata_bpr_loss_neg-both_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process: 0_metadata_bpr_loss_neg-both_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process: 0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process: 0_indicator_bpr_loss_neg-test_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_indicator_bpr_loss_neg-test_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process: 0_indicator_bpr_loss_neg-both_ld64_lr5_wd0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Initialized UserItemData '0_indicator_bpr_loss_neg-both_ld64_lr5_wd0' with dtype <class 'numpy.float32'>\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 9,908,333 positive interactions\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 103,052 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 1104 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Splitting interactions: train_pos=0.80, train_neg=0.80, random_state=None\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - No negative interactions to split\n",
      "UserItemData.0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Train/test split completed. Pos train: 7,926,717, Pos test: 1,981,616\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 9,830,156 negative interactions\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 9,830,156 negative interactions\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemData(0_metadata_bpr_loss_neg-ignore_ld64_lr5_wd0)\n",
      "  Fuser     :(138493×138493) nnz=   138,493 (0.001%), empty rows/cols=     0/     0\n",
      "  Fitem     :( 10160×  1104) nnz=   103,052 (0.919%), empty rows/cols=     0/     0\n",
      "  Rpos      :(138493× 10160) nnz= 9,908,333 (0.704%), empty rows/cols=   206/    30\n",
      "    └─ users: min=1, max=3028 | items: min=1, max=55807\n",
      "  Rneg      :(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n",
      "  Rpos_train:(138493× 10160) nnz= 7,926,717 (0.563%), empty rows/cols=   206/    33\n",
      "    └─ users: min=1, max=2423 | items: min=1, max=44687\n",
      "  Rpos_test :(138493× 10160) nnz= 1,981,616 (0.141%), empty rows/cols=  4007/   220\n",
      "    └─ users: min=1, max=605 | items: min=1, max=11120\n",
      "  Rneg_train:(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n",
      "  Rneg_test :(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Adding 10,160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 10160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Splitting interactions: train_pos=0.80, train_neg=0.80, random_state=None\n",
      "INFO - Initiating Hybrid Recommender System..\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added positive interactions. New dimensions: 138287 users × 10130 items\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 9,830,156 negative interactions\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - No negative interactions to split\n",
      "UserItemData.0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0 - INFO - Train/test split completed. Pos train: 7,926,742, Pos test: 1,981,591\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 9,830,156 negative interactions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemData(0_indicator_bpr_loss_neg-ignore_ld64_lr5_wd0)\n",
      "  Fuser     :(138493×138493) nnz=   138,493 (0.001%), empty rows/cols=     0/     0\n",
      "  Fitem     :( 10160× 10160) nnz=    10,160 (0.010%), empty rows/cols=     0/     0\n",
      "  Rpos      :(138493× 10160) nnz= 9,908,333 (0.704%), empty rows/cols=   206/    30\n",
      "    └─ users: min=1, max=3028 | items: min=1, max=55807\n",
      "  Rneg      :(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n",
      "  Rpos_train:(138493× 10160) nnz= 7,926,742 (0.563%), empty rows/cols=   206/    34\n",
      "    └─ users: min=1, max=2440 | items: min=1, max=44751\n",
      "  Rpos_test :(138493× 10160) nnz= 1,981,591 (0.141%), empty rows/cols=  3965/   235\n",
      "    └─ users: min=1, max=588 | items: min=1, max=11056\n",
      "  Rneg_train:(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n",
      "  Rneg_test :(138493× 10160) nnz=         0 (0.000%), empty rows/cols=138493/ 10160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Initiating Hybrid Recommender System..\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added negative interactions. New dimensions: 138493 users × 10150 items\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 103,052 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 1104 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added negative interactions. New dimensions: 138493 users × 10150 items\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 103,052 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 1104 item features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Splitting interactions: train_pos=0.80, train_neg=0.80, random_state=None\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added negative interactions. New dimensions: 138493 users × 10150 items\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Adding 10,160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-test_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 10160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added negative interactions. New dimensions: 138493 users × 10150 items\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 138,493 user features\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Split 9,830,156 negative interactions\n",
      "UserItemData.0_metadata_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Train/test split completed. Pos train: 7,926,727, Pos test: 1,981,606\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - 138493 users × 138493 user features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Adding 10,160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Successfully added item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - New dimensions: 10160 items × 10160 item features\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Splitting interactions: train_pos=0.80, train_neg=0.80, random_state=None\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Split 9,830,156 negative interactions\n",
      "UserItemData.0_indicator_bpr_loss_neg-both_ld64_lr5_wd0 - INFO - Train/test split completed. Pos train: 7,926,726, Pos test: 1,981,607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemData(0_metadata_bpr_loss_neg-both_ld64_lr5_wd0)\n",
      "  Fuser     :(138493×138493) nnz=   138,493 (0.001%), empty rows/cols=     0/     0\n",
      "  Fitem     :( 10160×  1104) nnz=   103,052 (0.919%), empty rows/cols=     0/     0\n",
      "  Rpos      :(138493× 10160) nnz= 9,908,333 (0.704%), empty rows/cols=   206/    30\n",
      "    └─ users: min=1, max=3028 | items: min=1, max=55807\n",
      "  Rneg      :(138493× 10160) nnz= 9,830,156 (0.699%), empty rows/cols=   239/    12\n",
      "    └─ users: min=1, max=4461 | items: min=1, max=27737\n",
      "  Rpos_train:(138493× 10160) nnz= 7,926,727 (0.563%), empty rows/cols=   206/    38\n",
      "    └─ users: min=1, max=2425 | items: min=1, max=44450\n",
      "  Rpos_test :(138493× 10160) nnz= 1,981,606 (0.141%), empty rows/cols=  3946/   240\n",
      "    └─ users: min=1, max=603 | items: min=1, max=11357\n",
      "  Rneg_train:(138493× 10160) nnz= 7,864,217 (0.559%), empty rows/cols=   239/    14\n",
      "    └─ users: min=1, max=3556 | items: min=1, max=22207\n",
      "  Rneg_test :(138493× 10160) nnz= 1,965,939 (0.140%), empty rows/cols=  6734/    55\n",
      "    └─ users: min=1, max=905 | items: min=1, max=5530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Initiating Hybrid Recommender System..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemData(0_indicator_bpr_loss_neg-both_ld64_lr5_wd0)\n",
      "  Fuser     :(138493×138493) nnz=   138,493 (0.001%), empty rows/cols=     0/     0\n",
      "  Fitem     :( 10160× 10160) nnz=    10,160 (0.010%), empty rows/cols=     0/     0\n",
      "  Rpos      :(138493× 10160) nnz= 9,908,333 (0.704%), empty rows/cols=   206/    30\n",
      "    └─ users: min=1, max=3028 | items: min=1, max=55807\n",
      "  Rneg      :(138493× 10160) nnz= 9,830,156 (0.699%), empty rows/cols=   239/    12\n",
      "    └─ users: min=1, max=4461 | items: min=1, max=27737\n",
      "  Rpos_train:(138493× 10160) nnz= 7,926,726 (0.563%), empty rows/cols=   206/    40\n",
      "    └─ users: min=1, max=2456 | items: min=1, max=44610\n",
      "  Rpos_test :(138493× 10160) nnz= 1,981,607 (0.141%), empty rows/cols=  4004/   214\n",
      "    └─ users: min=1, max=572 | items: min=1, max=11197\n",
      "  Rneg_train:(138493× 10160) nnz= 7,864,210 (0.559%), empty rows/cols=   239/    15\n",
      "    └─ users: min=1, max=3558 | items: min=1, max=22191\n",
      "  Rneg_test :(138493× 10160) nnz= 1,965,946 (0.140%), empty rows/cols=  6765/    59\n",
      "    └─ users: min=1, max=903 | items: min=1, max=5546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Initiating Hybrid Recommender System..\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(params, rdf, tdf):\n",
    "    \"\"\"Run a single experiment with the given parameters.\"\"\"\n",
    "    try:\n",
    "        recommender = build_recsys(rdf, tdf, **params)\n",
    "        return {\"status\": \"success\", \"recommender\": recommender, \"params\": params}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"failed\", \"error\": str(e), \"params\": params}\n",
    "\n",
    "\n",
    "# Create a partial function with fixed rdf and tdf\n",
    "run_with_fixed_data = partial(run_experiment, rdf=rdf, tdf=tdf)\n",
    "\n",
    "# Set the number of processes to use\n",
    "num_processes = min(len(all_params), os.cpu_count())\n",
    "print(f\"Using {num_processes} processes\")\n",
    "\n",
    "# Run experiments in parallel using pathos (supports dill serialization)\n",
    "pool = ProcessPool(nodes=num_processes)\n",
    "results = pool.map(run_with_fixed_data, all_params)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log results to MLflow\n",
    "\n",
    "After parallel training completes, we log each experiment's parameters, metrics, and artifacts to the HERO ML Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED (failed): metadata_neg-ignore_bpr_loss - [Errno 30] Read-only file system: '/kfs2'\n",
      "SKIPPED (failed): metadata_neg-test_bpr_loss - train_ratio_neg must be between 0 and 1, got 0.0\n",
      "SKIPPED (failed): metadata_neg-both_bpr_loss - [Errno 30] Read-only file system: '/kfs2'\n",
      "SKIPPED (failed): indicator_neg-ignore_bpr_loss - [Errno 30] Read-only file system: '/kfs2'\n",
      "SKIPPED (failed): indicator_neg-test_bpr_loss - train_ratio_neg must be between 0 and 1, got 0.0\n",
      "SKIPPED (failed): indicator_neg-both_bpr_loss - [Errno 30] Read-only file system: '/kfs2'\n",
      "\n",
      "All experiments logged to experiment: MovieLens BPR Experiments\n",
      "Tracking URI: https://dev-hero.nrel.gov/ml-model-registry/api/v1/proxy/dev-recommender-system-app\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    params = result[\"params\"]\n",
    "    run_name = f\"{params['item_option']}_{params['neg_option']}_{params['loss_function'].__name__}\"\n",
    "\n",
    "    if result[\"status\"] == \"failed\":\n",
    "        print(f\"SKIPPED (failed): {run_name} - {result['error']}\")\n",
    "        continue\n",
    "\n",
    "    recommender = result[\"recommender\"]\n",
    "    metrics_list = recommender.metrics\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        experiment_id=experiment.experiment_id,\n",
    "        run_name=run_name,\n",
    "    ):\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"item_option\": params[\"item_option\"],\n",
    "            \"neg_option\": params[\"neg_option\"],\n",
    "            \"n_latent\": params[\"n_latent\"],\n",
    "            \"learning_rate\": params[\"learning_rate\"],\n",
    "            \"weight_decay\": params[\"weight_decay\"],\n",
    "            \"loss_function\": params[\"loss_function\"].__name__,\n",
    "            \"run\": params[\"run\"],\n",
    "        })\n",
    "\n",
    "        # Log final epoch metrics\n",
    "        final = metrics_list[-1]\n",
    "        for key in [\"loss\", \"train_auc\", \"train_auc_std\", \"train_loss\",\n",
    "                     \"test_auc\", \"test_auc_std\", \"test_loss\"]:\n",
    "            if key in final:\n",
    "                mlflow.log_metric(f\"final_{key}\", final[key])\n",
    "\n",
    "        # Log best test AUC across all epochs\n",
    "        test_aucs = [m[\"test_auc\"] for m in metrics_list if \"test_auc\" in m]\n",
    "        if test_aucs:\n",
    "            mlflow.log_metric(\"best_test_auc\", max(test_aucs))\n",
    "\n",
    "        # Log per-epoch metrics as step metrics\n",
    "        for m in metrics_list:\n",
    "            step = m[\"epoch\"]\n",
    "            mlflow.log_metric(\"loss\", m[\"loss\"], step=step)\n",
    "            for key in [\"train_auc\", \"test_auc\", \"train_loss\", \"test_loss\"]:\n",
    "                if key in m:\n",
    "                    mlflow.log_metric(key, m[key], step=step)\n",
    "\n",
    "        # Save and log metrics JSON as artifact\n",
    "        recommender.save_metrics()\n",
    "        mlflow.log_artifact(\n",
    "            os.path.join(recommender.output_dir, \"metrics.json\"),\n",
    "            artifact_path=\"metrics\"\n",
    "        )\n",
    "\n",
    "        # Log best model checkpoint as artifact\n",
    "        model_files = [f for f in os.listdir(recommender.output_dir)\n",
    "                       if f.startswith(\"best_model\") and f.endswith(\".torch\")]\n",
    "        for mf in model_files:\n",
    "            mlflow.log_artifact(\n",
    "                os.path.join(recommender.output_dir, mf),\n",
    "                artifact_path=\"model\"\n",
    "            )\n",
    "\n",
    "    print(f\"Logged: {run_name}\")\n",
    "\n",
    "print(f\"\\nAll experiments logged to experiment: {experiment.name}\")\n",
    "print(f\"Tracking URI: {model_registry.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
