# Training Pipeline Configuration
# Used with TrainingPipeline for recommendation model training

# Model architecture
model:
  n_latent: 64                    # Latent factors/dimensions
  use_user_bias: true             # User bias terms
  use_global_bias: true           # Global bias term
  dropout: 0.0                    # Dropout rate
  activation: null                # Activation ('relu', 'tanh', 'sigmoid')
  sparse: false                   # Sparse gradients for embeddings

# Optimizer
optimizer:
  name: 'Adam'                     # 'Adam', 'SGD', 'AdamW', 'RMSprop'
  lr: 0.001                        # Learning rate
  weight_decay: 0.0001               # L2 regularization

# Training
training:
  loss_function: 'bpr_loss'       # Loss function
  n_iter: 400                     # Training iterations
  batch_size: 100                # Batch size
  eval_every: 10                   # Evaluation frequency
  eval_user_size: null            # Users to evaluate (null = all)
  eval_auc_neg_ratio: -1          # AUC neg ratio: -1=all items, 1.0=same
  early_stopping_patience: 1000     # Early stopping patience
  log_level: 1                    # Logging verbosity

# Data processing
data:
  item_feature: 'indicator'        # 'metadata', 'indicator', 'both'
  train_ratio_pos: 0.8            # Positive train ratio
  train_ratio_neg: 0.0           # Negative train ratio (0.0=test only)

# MLflow
mlflow:
  experiment_name: 'movielens'  # Experiment name
  tracking_uri: 'sqlite:///mlflow.db'    # Tracking URI

# Parameter sweep configuration (basic)
# sweep:
#   model.n_latent: [32, 64, 128]
#   optimizer.lr: [0.001, 0.01]
#   training.loss_function: ['bpr_loss', 'hinge_loss']

# Comprehensive parameter sweep (all relevant parameters)
sweep:
  model.n_latent: [32, 64, 128, 256]
  optimizer.lr: [0.0001, 0.001]
  training.loss_function: ['bpr_loss', 'hinge_loss']
  data.item_feature: ['metadata', 'indicator', 'both']
  data.train_ratio_neg: [0.0, 0.5, 0.8, 1.0]
