{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# MLflow Plotter Demo - visualize experiment results\nfrom pybpr.plotter import MLflowPlotter\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plotter with path to mlflow.db\n",
    "plotter = MLflowPlotter(tracking_uri=\"mlflow.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "experiments = plotter.get_experiments()\n",
    "print(\"Available experiments:\")\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runs for a specific experiment\n",
    "exp_name = \"simple_pipeline_examples\"\n",
    "runs = plotter.get_runs(experiment_name=exp_name)\n",
    "print(f\"Runs in '{exp_name}':\")\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Create summary table with final metrics\nsummary = plotter.summary_table(\n    experiment_name=exp_name,\n    metrics=[\"test_auc\", \"train_loss\"],\n    params=[\"n_latent\", \"lr\"]\n)\nprint(\"Run Summary (sorted by test_auc):\")\nsummary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Plot all metrics for a single run with ±2σ bands on AUC\nif len(runs) > 0:\n    run_id = runs.iloc[0][\"run_id\"]\n    fig = plotter.plot_single_run(\n        run_id=run_id, \n        figsize=(14, 5),\n        std_width=2.0,\n        show_std=True\n    )\n    plt.show()\nelse:\n    print(\"No runs available to plot\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Compare all runs with train_loss and test_auc side by side\nfig = plotter.plot_runs_comparison(\n    experiment_name=exp_name,\n    metrics=[\"train_loss\", \"test_auc\"],\n    figsize=(14, 5),\n    std_width=1.0,\n    show_std=True\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Compare runs with different std_width\nfig = plotter.plot_runs_comparison(\n    experiment_name=exp_name,\n    metrics=[\"train_loss\", \"test_auc\"],\n    figsize=(14, 5),\n    std_width=2.0,\n    show_std=True\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Plot top 3 runs by test_auc with std bands\nfig = plotter.plot_best_runs(\n    experiment_name=exp_name,\n    metric=\"test_auc\",\n    n_best=3,\n    plot_metrics=[\"train_loss\", \"test_auc\"],\n    figsize=(14, 5),\n    std_width=2.0,\n    show_std=True\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Get metric history for custom analysis\nif len(runs) > 0:\n    run_id = runs.iloc[0][\"run_id\"]\n    histories = plotter.get_run_metrics_history(\n        run_id=run_id,\n        metric_keys=[\"train_loss\", \"test_auc\"]\n    )\n    print(\"Train Loss history:\")\n    print(histories[\"train_loss\"].head())\n    print(\"\\nTest AUC history:\")\n    print(histories[\"test_auc\"].head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze grid search experiment results\ngrid_exp_name = \"example4_grid_search\"\n\ntry:\n    grid_runs = plotter.get_runs(\n        experiment_name=grid_exp_name\n    )\n    print(f\"Grid search runs: {len(grid_runs)}\")\n    \n    # Create summary table\n    grid_summary = plotter.summary_table(\n        experiment_name=grid_exp_name,\n        metrics=[\"test_auc\", \"train_loss\"],\n        params=[\"n_latent\", \"lr\", \"loss_function\"]\n    )\n    print(\"\\nGrid search results:\")\n    display(grid_summary)\n    \n    # Plot comparison with std bands\n    fig = plotter.plot_runs_comparison(\n        experiment_name=grid_exp_name,\n        metrics=[\"train_loss\", \"test_auc\"],\n        figsize=(14, 5),\n        std_width=1.0,\n        show_std=True\n    )\n    plt.show()\n    \nexcept ValueError as e:\n    msg = f\"Grid search experiment not found: {e}\"\n    print(msg)\n    print(\"Run Example 4 from simple_pipeline_example.py\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Create and save plot to file\nfig = plotter.plot_runs_comparison(\n    experiment_name=exp_name,\n    metrics=[\"train_loss\", \"test_auc\"],\n    figsize=(14, 5),\n    std_width=1.5,\n    show_std=True\n)\n\n# Save to file\nfig.savefig(\n    \"experiment_comparison.png\",\n    dpi=300,\n    bbox_inches=\"tight\"\n)\nprint(\"Plot saved to experiment_comparison.png\")\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}